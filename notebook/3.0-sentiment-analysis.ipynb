{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch import nn, optim\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from transformers import BertConfig, BertForSequenceClassification, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "def count_param(module: nn.Module, trainable=False) -> int:\n",
    "    if trainable:\n",
    "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "\n",
    "def get_lr(optimizer: optim.Optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n",
    "\n",
    "\n",
    "def freeze_layers(layers: nn.Module) -> None:\n",
    "    for layer in layers.parameters():\n",
    "        layer.requires_grad = False\n",
    "\n",
    "\n",
    "def metrics_to_string(metric_dict: dict) -> str:\n",
    "    string_list = []\n",
    "    for key, value in metric_dict.items():\n",
    "        string_list.append(\"{}:{:.2f}\".format(key, value))\n",
    "    return \" \".join(string_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentSentimentDataset(Dataset):\n",
    "    # Static constant variable\n",
    "    LABEL2INDEX = {\"positive\": 0, \"neutral\": 1, \"negative\": 2}\n",
    "    INDEX2LABEL = {0: \"positive\", 1: \"neutral\", 2: \"negative\"}\n",
    "    NUM_LABELS = 3\n",
    "\n",
    "    def load_dataset(self, path):\n",
    "        df = pd.read_csv(path)\n",
    "        df.columns = [\"text\", \"sentiment\"]\n",
    "        df[\"sentiment\"] = df[\"sentiment\"].apply(lambda lab: self.LABEL2INDEX[lab])\n",
    "        return df\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path: str,\n",
    "        tokenizer: BertTokenizer,\n",
    "        no_special_token=False,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.data = self.load_dataset(dataset_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.no_special_token = no_special_token\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data.loc[index, :]\n",
    "        text, sentiment = data[\"text\"], data[\"sentiment\"]\n",
    "        subwords = self.tokenizer.encode(\n",
    "            text, add_special_tokens=not self.no_special_token\n",
    "        )\n",
    "        return np.array(subwords), np.array(sentiment), data[\"text\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentSentimentDataLoader(DataLoader):\n",
    "    def __init__(self, max_seq_len=512, *args, **kwargs):\n",
    "        super(DocumentSentimentDataLoader, self).__init__(*args, **kwargs)\n",
    "        self.collate_fn = self._collate_fn\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def _collate_fn(self, batch):\n",
    "        batch_size = len(batch)\n",
    "        max_seq_len = max(map(lambda x: len(x[0]), batch))\n",
    "        max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "\n",
    "        subword_batch = np.zeros((batch_size, max_seq_len), dtype=np.int64)\n",
    "        mask_batch = np.zeros((batch_size, max_seq_len), dtype=np.float32)\n",
    "        sentiment_batch = np.zeros((batch_size, 1), dtype=np.int64)\n",
    "\n",
    "        seq_list = []\n",
    "        for i, (subwords, sentiment, raw_seq) in enumerate(batch):\n",
    "            subwords = subwords[:max_seq_len]\n",
    "            subword_batch[i, : len(subwords)] = subwords\n",
    "            mask_batch[i, : len(subwords)] = 1\n",
    "            sentiment_batch[i, 0] = sentiment\n",
    "\n",
    "            seq_list.append(raw_seq)\n",
    "\n",
    "        return subword_batch, mask_batch, sentiment_batch, seq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
    "config = BertConfig.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
    "config.num_labels = 3\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"indobenchmark/indobert-base-p1\", config=config\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_layers(model.bert.embeddings)\n",
    "freeze_layers(model.bert.encoder.layer[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(\n",
    "    model=model,\n",
    "    input_size=(1, 7),\n",
    "    dtypes=[torch.long],\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=16,\n",
    "    row_settings=[\"var_names\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../data/tws-train.csv\"\n",
    "test_path = \"../data/tws-test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DocumentSentimentDataset(train_path, tokenizer, lowercase=True)\n",
    "test_dataset = DocumentSentimentDataset(test_path, tokenizer, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "train_dataloader = DocumentSentimentDataLoader(\n",
    "    dataset=train_dataset,\n",
    "    max_seq_len=512,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DocumentSentimentDataLoader(\n",
    "    dataset=test_dataset,\n",
    "    max_seq_len=512,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\n",
    "w2i, i2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward function for sequence classification\n",
    "def forward_sequence_classification(\n",
    "    model: BertForSequenceClassification,\n",
    "    batch_data,\n",
    "    i2w,\n",
    "    is_test=False,\n",
    "    device=device,\n",
    "    **kwargs,\n",
    "):\n",
    "    # Unpack batch data\n",
    "    if len(batch_data) == 3:\n",
    "        (subword_batch, mask_batch, label_batch) = batch_data\n",
    "        token_type_batch = None\n",
    "    elif len(batch_data) == 4:\n",
    "        (subword_batch, mask_batch, token_type_batch, label_batch) = batch_data\n",
    "\n",
    "    # Prepare input & label\n",
    "    subword_batch = torch.LongTensor(subword_batch)\n",
    "    mask_batch = torch.FloatTensor(mask_batch)\n",
    "    token_type_batch = (\n",
    "        torch.LongTensor(token_type_batch) if token_type_batch is not None else None\n",
    "    )\n",
    "    label_batch = torch.LongTensor(label_batch)\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        subword_batch = subword_batch.cuda()\n",
    "        mask_batch = mask_batch.cuda()\n",
    "        token_type_batch = (\n",
    "            token_type_batch.cuda() if token_type_batch is not None else None\n",
    "        )\n",
    "        label_batch = label_batch.cuda()\n",
    "\n",
    "    # Forward model\n",
    "    outputs = model(\n",
    "        subword_batch,\n",
    "        attention_mask=mask_batch,\n",
    "        token_type_ids=token_type_batch,\n",
    "        labels=label_batch,\n",
    "    )\n",
    "    loss, logits = outputs[:2]\n",
    "\n",
    "    # generate prediction & label list\n",
    "    list_hyp = []\n",
    "    list_label = []\n",
    "    hyp = torch.topk(logits, 1)[1]\n",
    "    for j in range(len(hyp)):\n",
    "        list_hyp.append(i2w[hyp[j].item()])\n",
    "        list_label.append(i2w[label_batch[j][0].item()])\n",
    "\n",
    "    return loss, list_hyp, list_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_sentiment_metrics_fn(list_hyp, list_label):\n",
    "    metrics = {}\n",
    "    metrics[\"ACC\"] = accuracy_score(list_label, list_hyp)\n",
    "    metrics[\"F1\"] = f1_score(list_label, list_hyp, average=\"macro\")\n",
    "    metrics[\"REC\"] = recall_score(list_label, list_hyp, average=\"macro\")\n",
    "    metrics[\"PRE\"] = precision_score(list_label, list_hyp, average=\"macro\")\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=3e-6,\n",
    "    weight_decay=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    total_train_loss = 0\n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    train_pbar = tqdm(train_dataloader, leave=True, total=len(train_dataloader))\n",
    "    for i, batch_data in enumerate(train_pbar):\n",
    "        # Forward model\n",
    "        loss, batch_hyp, batch_label = forward_sequence_classification(\n",
    "            model, batch_data[:-1], i2w=i2w, device=\"cuda\"\n",
    "        )\n",
    "\n",
    "        # Update model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss = total_train_loss + loss.item()\n",
    "\n",
    "        # Calculate metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "\n",
    "    # Calculate train metric\n",
    "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "    train_pbar.set_description(\n",
    "        f\"(Epoch {epoch+1}) TRAIN LOSS:{total_train_loss / (i+1):.4f} {metrics} LR:{get_lr(optimizer):.8f}\"\n",
    "    )\n",
    "\n",
    "    # Evaluate on validation\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_correct, total_labels = 0, 0, 0\n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    test_pbar = tqdm(test_dataloader, leave=True, total=len(test_dataloader))\n",
    "    with torch.inference_mode():\n",
    "        for i, batch_data in enumerate(test_pbar):\n",
    "            batch_seq = batch_data[-1]\n",
    "            loss, batch_hyp, batch_label = forward_sequence_classification(\n",
    "                model, batch_data[:-1], i2w=i2w, device=\"cuda\"\n",
    "            )\n",
    "\n",
    "            # Calculate total loss\n",
    "            valid_loss = loss.item()\n",
    "            total_loss = total_loss + valid_loss\n",
    "\n",
    "            # Calculate evaluation metrics\n",
    "            list_hyp += batch_hyp\n",
    "            list_label += batch_label\n",
    "\n",
    "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "    test_pbar.set_description(\n",
    "        f\"(Epoch {epoch+1}) VALID LOSS:{total_loss / (i+1):.4f} {metrics}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(save_directory=\"indobert-sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    total_loss, total_correct, total_labels = 0, 0, 0\n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    pbar = tqdm(test_dataloader, leave=True, total=len(test_dataloader))\n",
    "    for i, batch_data in enumerate(pbar):\n",
    "        _, batch_hyp, batch_label = forward_sequence_classification(\n",
    "            model, batch_data[:-1], i2w=i2w, device=\"cuda\"\n",
    "        )\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "\n",
    "# Save prediction\n",
    "results_df = pd.DataFrame({\"sentiment\": list_hyp, \"label\": list_label}).reset_index(\n",
    "    drop=True\n",
    ")\n",
    "results_df.to_csv(\"../data/pred.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
